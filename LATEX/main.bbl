% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{empty000}{misc}{}
      \name{author}{1}{}{%
        {{hash=2ecd214db016e0ad63452ae4236e065d}{%
           family={JD},
           familyi={J\bibinitperiod}}}%
      }
      \strng{namehash}{2ecd214db016e0ad63452ae4236e065d}
      \strng{fullhash}{2ecd214db016e0ad63452ae4236e065d}
      \strng{bibnamehash}{2ecd214db016e0ad63452ae4236e065d}
      \strng{authorbibnamehash}{2ecd214db016e0ad63452ae4236e065d}
      \strng{authornamehash}{2ecd214db016e0ad63452ae4236e065d}
      \strng{authorfullhash}{2ecd214db016e0ad63452ae4236e065d}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Placeholder citation}
      \field{year}{0}
    \endentry
    \entry{vaswani2023}{misc}{}
      \name{author}{8}{}{%
        {{hash=7f28e84700536646dd6620a0db07ad09}{%
           family={Vaswani},
           familyi={V\bibinitperiod},
           given={Ashish},
           giveni={A\bibinitperiod}}}%
        {{hash=62efade83d70f0323fe248755e6c90c5}{%
           family={Shazeer},
           familyi={S\bibinitperiod},
           given={Noam},
           giveni={N\bibinitperiod}}}%
        {{hash=06649ebab1ea5cac0250746a19764975}{%
           family={Parmar},
           familyi={P\bibinitperiod},
           given={Niki},
           giveni={N\bibinitperiod}}}%
        {{hash=831027ee0ebf22375e2a86afc1881909}{%
           family={Uszkoreit},
           familyi={U\bibinitperiod},
           given={Jakob},
           giveni={J\bibinitperiod}}}%
        {{hash=2fd2982e30ebcec93ec1cf76e0d797fd}{%
           family={Jones},
           familyi={J\bibinitperiod},
           given={Llion},
           giveni={L\bibinitperiod}}}%
        {{hash=27b07e4eacbf4ef7a1438e3badb7dd8d}{%
           family={Gomez},
           familyi={G\bibinitperiod},
           given={Aidan\bibnamedelima N.},
           giveni={A\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=f2bc899b1160163417da7bf510f15d33}{%
           family={Kaiser},
           familyi={K\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=95595a0fefb86187cbc36e551017d332}{%
           family={Polosukhin},
           familyi={P\bibinitperiod},
           given={Illia},
           giveni={I\bibinitperiod}}}%
      }
      \strng{namehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{fullhash}{f82970bbd2bdd7a002d2af62b743d5cc}
      \strng{bibnamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authorbibnamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authornamehash}{7cc810834e9ba19dd5f8b0a37be3172d}
      \strng{authorfullhash}{f82970bbd2bdd7a002d2af62b743d5cc}
      \field{sortinit}{4}
      \field{sortinithash}{9381316451d1b9788675a07e972a12a7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.CL}
      \field{eprinttype}{arXiv}
      \field{title}{Attention Is All You Need}
      \field{year}{2023}
      \verb{eprint}
      \verb 1706.03762
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/1706.03762
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/1706.03762
      \endverb
    \endentry
    \entry{chen2024}{article}{}
      \name{author}{3}{}{%
        {{hash=dd060cd064f8f607eb1bee6712820270}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Hao},
           giveni={H\bibinitperiod}}}%
        {{hash=b2bc6ab135e4aae028003d3234dba15c}{%
           family={Flores},
           familyi={F\bibinitperiod},
           given={Gonzalo\bibnamedelimb E.\bibnamedelimi Constante},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=8e6ea9d2aeafe6a7426446dd92acf0ff}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Can},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{fbae51f7fc747eb183ea2edc36b29e95}
      \strng{fullhash}{fbae51f7fc747eb183ea2edc36b29e95}
      \strng{bibnamehash}{fbae51f7fc747eb183ea2edc36b29e95}
      \strng{authorbibnamehash}{fbae51f7fc747eb183ea2edc36b29e95}
      \strng{authornamehash}{fbae51f7fc747eb183ea2edc36b29e95}
      \strng{authorfullhash}{fbae51f7fc747eb183ea2edc36b29e95}
      \field{sortinit}{6}
      \field{sortinithash}{b33bc299efb3c36abec520a4c896a66d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Surrogate modeling is used to replace computationally expensive simulations. Neural networks have been widely applied as surrogate models that enable efficient evaluations over complex physical systems. Despite this, neural networks are data-driven models and devoid of any physics. The incorporation of physics into neural networks can improve generalization and data efficiency. The physics-informed neural network (PINN) is an approach to leverage known physical constraints present in the data, but it cannot strictly satisfy them in the predictions. This work proposes a novel physics-informed neural network, KKT-hPINN, which rigorously guarantees hard linear equality constraints through projection layers derived from KKT conditions. Numerical experiments on Aspen models of a continuous stirred-tank reactor (CSTR) unit, an extractive distillation subsystem, and a chemical plant demonstrate that this model can further enhance the prediction accuracy.}
      \field{issn}{0098-1354}
      \field{journaltitle}{Computers \& Chemical Engineering}
      \field{title}{Physics-informed neural networks with hard linear equality constraints}
      \field{volume}{189}
      \field{year}{2024}
      \field{pages}{108764}
      \range{pages}{1}
      \verb{doi}
      \verb https://doi.org/10.1016/j.compchemeng.2024.108764
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0098135424001820
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0098135424001820
      \endverb
      \keyw{Surrogate modeling,Physics-informed neural network,Artificial intelligence}
    \endentry
    \entry{houska2011}{thesis}{}
      \name{author}{1}{}{%
        {{hash=b41ef4ad86190f67f0963267eef061a4}{%
           family={Houska},
           familyi={H\bibinitperiod},
           given={B.},
           giveni={B\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {KU Leuven}%
      }
      \strng{namehash}{b41ef4ad86190f67f0963267eef061a4}
      \strng{fullhash}{b41ef4ad86190f67f0963267eef061a4}
      \strng{bibnamehash}{b41ef4ad86190f67f0963267eef061a4}
      \strng{authorbibnamehash}{b41ef4ad86190f67f0963267eef061a4}
      \strng{authornamehash}{b41ef4ad86190f67f0963267eef061a4}
      \strng{authorfullhash}{b41ef4ad86190f67f0963267eef061a4}
      \field{sortinit}{7}
      \field{sortinithash}{108d0be1b1bee9773a1173443802c0a3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-94-6018-394-2}
      \field{title}{Robust Optimization of Dynamic Systems}
      \field{type}{phdthesis}
      \field{year}{2011}
    \endentry
    \entry{stein2003}{inbook}{}
      \name{author}{1}{}{%
        {{hash=e3ad00a456f8b735e402f1a0e7caf74c}{%
           family={Stein},
           familyi={S\bibinitperiod},
           given={Oliver},
           giveni={O\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Boston, MA}%
      }
      \list{publisher}{1}{%
        {Springer US}%
      }
      \strng{namehash}{e3ad00a456f8b735e402f1a0e7caf74c}
      \strng{fullhash}{e3ad00a456f8b735e402f1a0e7caf74c}
      \strng{bibnamehash}{e3ad00a456f8b735e402f1a0e7caf74c}
      \strng{authorbibnamehash}{e3ad00a456f8b735e402f1a0e7caf74c}
      \strng{authornamehash}{e3ad00a456f8b735e402f1a0e7caf74c}
      \strng{authorfullhash}{e3ad00a456f8b735e402f1a0e7caf74c}
      \field{sortinit}{9}
      \field{sortinithash}{0a5ebc79d83c96b6579069544c73c7d4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{From our point of view, a main obstruction in the numerical treatment of semiinfinite optimization problems is that one needs to find global solutions of the lower level problems. As it is well-known, numerically one can at most find local solutions or stationary points unless a certain global problem structure is given. A standard problem structure which global optimization methods exploit is convexity, so that we develop our numerical approach for convex lower level problems. We stress that this convexity assumption with respect to the index variable y is not related to the usual convexity assumption in standard semi-infinite programming, where the upper level defining functions have to be convex with respect to the decision variable x (cf., e.g., [2, 62, 118]). Analogously, in linear semi-infinite programming the linearity is always assumed with respect to the upper level problem.}
      \field{booktitle}{Bi-Level Strategies in Semi-Infinite Programming}
      \field{isbn}{978-1-4419-9164-5}
      \field{title}{Bi-Level Methods for Semi-Infinite Programming}
      \field{year}{2003}
      \field{pages}{145\bibrangedash 169}
      \range{pages}{25}
      \verb{doi}
      \verb 10.1007/978-1-4419-9164-5_5
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1007/978-1-4419-9164-5_5
      \endverb
      \verb{url}
      \verb https://doi.org/10.1007/978-1-4419-9164-5_5
      \endverb
    \endentry
    \entry{hornik1990}{article}{}
      \name{author}{1}{}{%
        {{hash=bec8dd1340457a82c21b54971e4c9417}{%
           family={Hornik},
           familyi={H\bibinitperiod},
           given={Kurt},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{bec8dd1340457a82c21b54971e4c9417}
      \strng{fullhash}{bec8dd1340457a82c21b54971e4c9417}
      \strng{bibnamehash}{bec8dd1340457a82c21b54971e4c9417}
      \strng{authorbibnamehash}{bec8dd1340457a82c21b54971e4c9417}
      \strng{authornamehash}{bec8dd1340457a82c21b54971e4c9417}
      \strng{authorfullhash}{bec8dd1340457a82c21b54971e4c9417}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.}
      \field{issn}{0893-6080}
      \field{journaltitle}{Neural Networks}
      \field{number}{2}
      \field{title}{Approximation capabilities of multilayer feedforward networks}
      \field{volume}{4}
      \field{year}{1991}
      \field{pages}{251\bibrangedash 257}
      \range{pages}{7}
      \verb{doi}
      \verb https://doi.org/10.1016/0893-6080(91)90009-T
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/089360809190009T
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/089360809190009T
      \endverb
      \keyw{Multilayer feedforward networks,Activation function,Universal approximation capabilities,Input environment measure,() approximation,Uniform approximation,Sobolev spaces,Smooth approximation}
    \endentry
    \entry{salzmann2023}{article}{}
      \name{author}{6}{}{%
        {{hash=eb27cafdba112e389cc0cb75baa97b7b}{%
           family={Salzmann},
           familyi={S\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=67c97d80868fd6b133dedb6e929aac2d}{%
           family={Kaufmann},
           familyi={K\bibinitperiod},
           given={Elia},
           giveni={E\bibinitperiod}}}%
        {{hash=292b36afab7a67913e42da468e04d6b4}{%
           family={Arrizabalaga},
           familyi={A\bibinitperiod},
           given={Jon},
           giveni={J\bibinitperiod}}}%
        {{hash=35318fcd7c56edabda1a4c70f2174048}{%
           family={Pavone},
           familyi={P\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=dac21ab4ede215439fcc6b051be53a11}{%
           family={Scaramuzza},
           familyi={S\bibinitperiod},
           given={Davide},
           giveni={D\bibinitperiod}}}%
        {{hash=4b3591f69e427acf5acb3a54bc4ac44d}{%
           family={Ryll},
           familyi={R\bibinitperiod},
           given={Markus},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{2}{%
        {Institute of Electrical}%
        {Electronics Engineers (IEEE)}%
      }
      \strng{namehash}{41cb005ff0302918e0593691eef05c38}
      \strng{fullhash}{7488356e9a821e89b0ce80c8d0cca966}
      \strng{bibnamehash}{7488356e9a821e89b0ce80c8d0cca966}
      \strng{authorbibnamehash}{7488356e9a821e89b0ce80c8d0cca966}
      \strng{authornamehash}{41cb005ff0302918e0593691eef05c38}
      \strng{authorfullhash}{7488356e9a821e89b0ce80c8d0cca966}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{2377-3774}
      \field{journaltitle}{IEEE Robotics and Automation Letters}
      \field{month}{4}
      \field{note}{Signifikante Beschleunigung der MPC backward differentiation des NNs durch eine Taylorreihe zweiter Ordnung.}
      \field{number}{4}
      \field{title}{Real-Time Neural MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms}
      \field{volume}{8}
      \field{year}{2023}
      \field{pages}{2397\bibrangedash 2404}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1109/lra.2023.3246839
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1109/LRA.2023.3246839
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1109/LRA.2023.3246839
      \endverb
    \endentry
    \entry{lucia2013}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=14d57392f0fda695a4a1f6fb6143073b}{%
           family={Lucia},
           familyi={L\bibinitperiod},
           given={Sergio},
           giveni={S\bibinitperiod}}}%
        {{hash=f593c290c432f488e6232f627610ddaa}{%
           family={Engell},
           familyi={E\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{fullhash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{bibnamehash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{authorbibnamehash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{authornamehash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{authorfullhash}{3081569fb9bd57eab75348226e74fe2f}
      \field{extraname}{1}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{2013 European Control Conference (ECC)}
      \field{title}{Robust nonlinear model predictive control of a batch bioreactor using multi-stage stochastic programming}
      \field{year}{2013}
      \field{pages}{4124\bibrangedash 4129}
      \range{pages}{6}
      \verb{doi}
      \verb 10.23919/ECC.2013.6669521
      \endverb
      \keyw{Uncertainty;Optimization;Robustness;Silicon compounds;Biomass;Substrates;Standards}
    \endentry
    \entry{biegler2010}{inbook}{}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labeltitlesource}{title}
      \field{abstract}{This chapter develops dynamic optimization strategies that couple solvers for differential algebraic equations (DAEs) with NLP algorithms developed in the previous chapters. First, to provide a better understanding of DAE solvers, widely used Runge—Kutta and linear multistep methods are described, and a summary of their properties is given. A framework is then described for sequential dynamic optimization approaches that rely on DAE solvers to provide function information to the NLP solver. In addition, gradient information is provided by direct and adjoint sensitivity methods. Both methods are derived and described along with examples. Unfortunately, the sequential approach can fail on unstable and ill-conditioned systems. To deal with this case, we extend the sequential approach through the use of multiple shooting and concepts of boundary value solvers. Finally, a process case study is presented that illustrates both sequential and multiple shooting approaches. 9.1 Introduction Chapter 8 provided an overview of the features of dynamic optimization problems and their optimality conditions. From that chapter, it is clear that the extension to larger dynamic optimization problems requires efficient and reliable numerical algorithms. This chapter addresses this question by developing a popular approach for the optimization of dynamic process systems. Powerful DAE solvers for large-scale initial value problems have led to widely used simulation environments for dynamic nonlinear processes. The ability to develop dynamic process simulation models naturally leads to their extension for optimization studies. Moreover, with the development of reliable and efficient NLP solvers, discussed in Chapter 6, an optimization capability can be implemented for dynamic systems along the lines of modular optimization modes discussed in Chapter 7. With robust simulation models, the implementation of NLP codes can be done in a reasonably straightforward way.}
      \field{booktitle}{Nonlinear Programming}
      \field{chapter}{9}
      \field{title}{Dynamic Optimization Methods with Embedded DAE Solvers}
      \field{pages}{251\bibrangedash 286}
      \range{pages}{36}
      \verb{doi}
      \verb 10.1137/1.9780898719383.ch9
      \endverb
      \verb{eprint}
      \verb https://epubs.siam.org/doi/pdf/10.1137/1.9780898719383.ch9
      \endverb
      \verb{urlraw}
      \verb https://epubs.siam.org/doi/abs/10.1137/1.9780898719383.ch9
      \endverb
      \verb{url}
      \verb https://epubs.siam.org/doi/abs/10.1137/1.9780898719383.ch9
      \endverb
    \endentry
    \entry{lucia2015}{article}{}
      \name{author}{2}{}{%
        {{hash=14d57392f0fda695a4a1f6fb6143073b}{%
           family={Lucia},
           familyi={L\bibinitperiod},
           given={Sergio},
           giveni={S\bibinitperiod}}}%
        {{hash=f593c290c432f488e6232f627610ddaa}{%
           family={Engell},
           familyi={E\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{fullhash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{bibnamehash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{authorbibnamehash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{authornamehash}{3081569fb9bd57eab75348226e74fe2f}
      \strng{authorfullhash}{3081569fb9bd57eab75348226e74fe2f}
      \field{extraname}{2}
      \field{sortinit}{1}
      \field{sortinithash}{4f6aaa89bab872aa0999fec09ff8e98a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multi-stage Nonlinear Model Predictive Control (NMPC) is a promising strategy for the design of robust NMPC controllers which is based on describing the evolution of the uncertainty as a scenario tree. The scenario tree makes it possible to consider explicitly that the future control inputs can be adapted to the future information (measurements), thus reducing the conservativeness of the robust approach. This paper reviews the multi-stage approach and illustrates its main advantages using a nonlinear CSTR example. We also provide guidelines for possible multi-stage NMPC users that could help to identify the problems where the use of multi-stage NMPC can result in a significant improvement with respect to standard NMPC or other robust NMPC approaches. Finally, we summarize the different modifications that can be done to the multi-stage approach to enhance its performance. The possible enhancements include: improved performance using parameter estimation, rigorous guarantee of constraint satisfaction, and stability guarantees for the case of discrete-valued uncertainties.}
      \field{issn}{2405-8963}
      \field{journaltitle}{IFAC-PapersOnLine}
      \field{note}{9th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2015}
      \field{number}{8}
      \field{title}{Potential and Limitations of Multi-stage Nonlinear Model Predictive Control}
      \field{volume}{48}
      \field{year}{2015}
      \field{pages}{1015\bibrangedash 1020}
      \range{pages}{6}
      \verb{doi}
      \verb https://doi.org/10.1016/j.ifacol.2015.09.101
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S2405896315011829
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S2405896315011829
      \endverb
    \endentry
    \entry{mcbride2019}{article}{}
      \name{author}{2}{}{%
        {{hash=e6515f3c440e8dd9d2981516a1f1e0a3}{%
           family={McBride},
           familyi={M\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=a5148fca6c6bde4ba775cee31842e0b4}{%
           family={Sundmacher},
           familyi={S\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{fab19337b76dd8b564a4bc4b84d8e0a7}
      \strng{fullhash}{fab19337b76dd8b564a4bc4b84d8e0a7}
      \strng{bibnamehash}{fab19337b76dd8b564a4bc4b84d8e0a7}
      \strng{authorbibnamehash}{fab19337b76dd8b564a4bc4b84d8e0a7}
      \strng{authornamehash}{fab19337b76dd8b564a4bc4b84d8e0a7}
      \strng{authorfullhash}{fab19337b76dd8b564a4bc4b84d8e0a7}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Abstract The ability to accurately model and simulate chemical processes has been paramount to the growing success and efficiency in process design and operation. These improvements usually come with increasing complexity of the underlying models leading to substantial computational effort in their use. It may also occur that the structure of the model is sometimes unknown making optimization and study difficult. To circumvent these issues, mathematically simpler models, commonly known as surrogate models, have been designed and used to successfully replace these complex, underlying models with much success. This technique has seen increasing use within the chemical process engineering field and this article summarizes some popular surrogates and their recent use in this area.}
      \field{journaltitle}{Chemie Ingenieur Technik}
      \field{number}{3}
      \field{title}{Overview of Surrogate Modeling in Chemical Process Engineering}
      \field{volume}{91}
      \field{year}{2019}
      \field{pages}{228\bibrangedash 239}
      \range{pages}{12}
      \verb{doi}
      \verb https://doi.org/10.1002/cite.201800091
      \endverb
      \verb{eprint}
      \verb https://onlinelibrary.wiley.com/doi/pdf/10.1002/cite.201800091
      \endverb
      \verb{urlraw}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/cite.201800091
      \endverb
      \verb{url}
      \verb https://onlinelibrary.wiley.com/doi/abs/10.1002/cite.201800091
      \endverb
      \keyw{Process design,Process optimization,Simulations,Surrogate modeling}
    \endentry
    \entry{salzmann2024l4casadi}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=eb27cafdba112e389cc0cb75baa97b7b}{%
           family={Salzmann},
           familyi={S\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=292b36afab7a67913e42da468e04d6b4}{%
           family={Arrizabalaga},
           familyi={A\bibinitperiod},
           given={Jon},
           giveni={J\bibinitperiod}}}%
        {{hash=74a48f793c45b9c6c2afef7c942bd89c}{%
           family={Andersson},
           familyi={A\bibinitperiod},
           given={Joel},
           giveni={J\bibinitperiod}}}%
        {{hash=35318fcd7c56edabda1a4c70f2174048}{%
           family={Pavone},
           familyi={P\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=4b3591f69e427acf5acb3a54bc4ac44d}{%
           family={Ryll},
           familyi={R\bibinitperiod},
           given={Markus},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{56d723642dec0f1919bb8f8144c07938}
      \strng{fullhash}{bfff084478c488bab3fb34856cdab6dd}
      \strng{bibnamehash}{bfff084478c488bab3fb34856cdab6dd}
      \strng{authorbibnamehash}{bfff084478c488bab3fb34856cdab6dd}
      \strng{authornamehash}{56d723642dec0f1919bb8f8144c07938}
      \strng{authorfullhash}{bfff084478c488bab3fb34856cdab6dd}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Learning for Dynamics and Control Conference (L4DC)}
      \field{title}{Learning for CasADi: Data-driven Models in Numerical Optimization}
      \field{year}{2024}
    \endentry
    \entry{raissi2019}{article}{}
      \name{author}{3}{}{%
        {{hash=a3194cb4f8e3959e569236521aa2fd86}{%
           family={Raissi},
           familyi={R\bibinitperiod},
           given={M.},
           giveni={M\bibinitperiod}}}%
        {{hash=c2e8423b7b49343a85abf9af92ee2a82}{%
           family={Perdikaris},
           familyi={P\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod}}}%
        {{hash=1c2f5930e36485e8d38a87746769fdcc}{%
           family={Karniadakis},
           familyi={K\bibinitperiod},
           given={G.E.},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{fullhash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{bibnamehash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{authorbibnamehash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{authornamehash}{a17a0649dc8bad70026488d4ea37c508}
      \strng{authorfullhash}{a17a0649dc8bad70026488d4ea37c508}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
      \field{issn}{0021-9991}
      \field{journaltitle}{Journal of Computational Physics}
      \field{title}{Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations}
      \field{volume}{378}
      \field{year}{2019}
      \field{pages}{686\bibrangedash 707}
      \range{pages}{22}
      \verb{doi}
      \verb https://doi.org/10.1016/j.jcp.2018.10.045
      \endverb
      \verb{urlraw}
      \verb https://www.sciencedirect.com/science/article/pii/S0021999118307125
      \endverb
      \verb{url}
      \verb https://www.sciencedirect.com/science/article/pii/S0021999118307125
      \endverb
      \keyw{Data-driven scientific computing,Machine learning,Predictive modeling,Runge–Kutta methods,Nonlinear dynamics}
    \endentry
    \entry{cvxpylayers2019}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=432ef86ebc419d2ed3b9623b268fb496}{%
           family={Agrawal},
           familyi={A\bibinitperiod},
           given={A.},
           giveni={A\bibinitperiod}}}%
        {{hash=af93026df98992a920daa1ebda770ac9}{%
           family={Amos},
           familyi={A\bibinitperiod},
           given={B.},
           giveni={B\bibinitperiod}}}%
        {{hash=99c9deca43bfbd9ae125a281af52dc7f}{%
           family={Barratt},
           familyi={B\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
        {{hash=fdb760b740da8a080df12615e128118d}{%
           family={Boyd},
           familyi={B\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
        {{hash=fb1e477d61db9a572b81c2fb8148bdb2}{%
           family={Diamond},
           familyi={D\bibinitperiod},
           given={S.},
           giveni={S\bibinitperiod}}}%
        {{hash=9c0a601375a2eba4b1a7c962641cef5d}{%
           family={Kolter},
           familyi={K\bibinitperiod},
           given={Z.},
           giveni={Z\bibinitperiod}}}%
      }
      \strng{namehash}{2f66e877c58e6ee278092df3e9f963cc}
      \strng{fullhash}{3a2e73c8b36fe75431aaa17a99d757a4}
      \strng{bibnamehash}{3a2e73c8b36fe75431aaa17a99d757a4}
      \strng{authorbibnamehash}{3a2e73c8b36fe75431aaa17a99d757a4}
      \strng{authornamehash}{2f66e877c58e6ee278092df3e9f963cc}
      \strng{authorfullhash}{3a2e73c8b36fe75431aaa17a99d757a4}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in Neural Information Processing Systems}
      \field{title}{Differentiable Convex Optimization Layers}
      \field{year}{2019}
    \endentry
    \entry{lastrucci2025}{misc}{}
      \name{author}{4}{}{%
        {{hash=15fd7dd6da2445310aa00610f7b57637}{%
           family={Lastrucci},
           familyi={L\bibinitperiod},
           given={Giacomo},
           giveni={G\bibinitperiod}}}%
        {{hash=800ab7eacf599953004e502166e8dd3f}{%
           family={Karia},
           familyi={K\bibinitperiod},
           given={Tanuj},
           giveni={T\bibinitperiod}}}%
        {{hash=07f1b8ae15ae3b8370710f27bb3008f0}{%
           family={Gromotka},
           familyi={G\bibinitperiod},
           given={Zoë},
           giveni={Z\bibinitperiod}}}%
        {{hash=2b52d0cf2fd9795ade42fc27669466b9}{%
           family={Schweidtmann},
           familyi={S\bibinitperiod},
           given={Artur\bibnamedelima M.},
           giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{49313e475aea28e0c1cecaeeca32fac6}
      \strng{fullhash}{2104aea70c64086e1199f6b31029fccf}
      \strng{bibnamehash}{2104aea70c64086e1199f6b31029fccf}
      \strng{authorbibnamehash}{2104aea70c64086e1199f6b31029fccf}
      \strng{authornamehash}{49313e475aea28e0c1cecaeeca32fac6}
      \strng{authorfullhash}{2104aea70c64086e1199f6b31029fccf}
      \field{sortinit}{2}
      \field{sortinithash}{8b555b3791beccb63322c22f3320aa9a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{title}{Picard-KKT-hPINN: Enforcing Nonlinear Enthalpy Balances for Physically Consistent Neural Networks}
      \field{year}{2025}
      \verb{eprint}
      \verb 2501.17782
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/2501.17782
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/2501.17782
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

