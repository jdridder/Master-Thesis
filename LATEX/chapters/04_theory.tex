\chapter{Model Predictive Control under Uncertainty}


Before diving into the application of a special model predictive (MPC) algorithm. Uncertain
optimization is treated more generally. From economics, over self-driving cars to chemical plants,
optimization problems are formulated in terms of uncertain variables in countless variations.
These uncertainties arise in three different types – fluctuating disturbances such as wind or weather conditions,
parametric offsets such as wrongly determined friction coefficients or model mismatches like unseen side reactions.
As the straight maximization or minimzation of an objective often puts a problem to its boundaries,
unforeseen disturbances may lead to \emph{constraint violation} or even \emph{instability} \cite{houska2011}.
Therefore, \emph{robust} optimization formulations are needed, that are proof against mentioned challenges as
well as retain the \emph{performance} with respect to the objective function.
\newline

\section{Robust Optimization}

Considering the following optimization task (\ref{theroy:def: semi-infinite program}) under an uncertainty vector $\vw$.
\begin{definition} Semi-infinite program with the uncertainty $\vw$.
    \label{theroy:def: semi-infinite program}
    \[
    \begin{gathered}
        \underset{\vu}{\min} \; \underset{\vw}{\max} \quad F_0(\vu, \vw) \\
        \text{s.t.} \quad F_i(\vu, \vw) \leq 0 \quad \forall \vw \in \mathbb{W}
    \end{gathered}
    \]
\end{definition}

Here, the optimization of the objective is formulated in a bilevel form. The scalar objective function $F_0$ is to 
be minimized w.r.t the decision variables $\vu$ for the worst case scenario w.r.t the disturbance
vector $\vw$. It can be interpreted as a player-vs-player setup \cite{houska2011}. One player (the optimizer)
seeks a minimum solution for the input $\vu$, while the opposing player (nature) chooses a maximum solution for the
disturbance variables $\vw$. The inequality constraints $F_i$ in addition, should be satisfied for all possible disturbances, that
lie in a defined set $\mathbb{W}$. If this set is not finite, the number of possible 
constraints to be satisifed is infintely big. Thus, this formulation is named as \emph{semi-infinite-program} \cite{stein2003}.
To reduce the number of constraints, the worst-case estimate can also be applied here (\ref{theroy:def: bi-level program}). 

\begin{definition} Bi-level formulation of the semi-infinite program
    \label{theroy:def: bi-level program}
    \[
    \begin{gathered}
        \underset{\vu}{\min} \; \underset{\vw}{\max} \quad F_0(\vu, \vw) \\
        \text{s.t.} \quad \varphi_i(\vu) \coloneq \underset{\vw}{\max} \quad F_i(\vu, \vw) \leq 0
        \quad \vw \in \mathbb{W}
    \end{gathered}
    \]
\end{definition}

Now, the inequality constraints are also formulated in a bi-level manner. The maximum of $F_i$ is 
constant in $\vw$ and only a function of the decision variables $\vu$. The function $\varphi_i$
exactly describes the wost-case for $F_i$ parametrized by $\vu$. In fact, if certain assumptions hold
regarding the uncertainty set $\mathbb{W}$ and the concavity of the maximization,
$\varphi_i$ can be exactly computed \cite{empty000}. This would simplify the problem dramatically.
In reality however, an analytical solution for $\varphi_i$ is not possible to compute.
But with the universal approximation capability of neural networks (NNs) in mind \cite{hornik1990}, 
a data-driven approach is promising, that delivers an approximated function $\tilde{\varphi}_i(\vu)$.


\section{Model Predictive Control}

Model predictive control (MPC) is a type of optimization, in which an optimal control problem is solved
in a dynamic manner. The main goal of this control strategy is to find the best inputs to a system that
maximize or minimize a defined objective. In a chemical process, for instance, the energy consumption
for heating should be minimized. It should be minimized in such a way, that other variables remain 
within their bounds. For example, the product specification and, most importantly, the safety of the process
needs to be ensured under all circumstances. In contrast to the \emph{optimal design} of a process, that 
is done during the engineering phase, an MPC controller operates \emph{online}. It acts on the process
while it is running. Thus, it is coined \emph{dynamic optimization}. To do so, the optimization
problem needs to solved recursively at given time instants.
Chemical plants usually have
large \emph{time constants}, meaning, they react slowly to their inputs.
Other physical systems with low time constants, like quadrocopters behave much faster. So, the
computation time of an MPC algorithm is to be kept low \cite{salzmann2023} to be able to react to the system.
To predict the future behavior of the system, MPC requires a system model.
\newline
\newline
In total, an MPC controller is based on three pillars – an objective function, a set of constraints and, crucially,
the model of the system.

\begin{definition} Continuous-time formulation of an MPC problem.
    \label{theory:def: general mpc}
    \[
        \begin{gathered}
            \underset{\vu}{\min} \quad  J(t_0, t_\text{end}, \vx, \vu) \\
            \begin{aligned}
                \text{s.t.} \quad &\dot{\vx} = f(t, \vx, \vu) \\
                & F_i(t, \vx, \vu) \leq 0 \quad \forall t \in [t_0, t_\text{end}] \\
                & \vx_0 = \vx_\text{init}
            \end{aligned}
        \end{gathered}
    \]
\end{definition}

Optimal system inputs $\vu$ are to be determined, that minimize the objective function $J$. This objective
varies with the inputs and the states of the system $\vx$. It is to be minimized over a time horizon
from $t_0$ to $t_\text{end}$, called the \emph{prediction horizon}. The states of the system evolve along
the dynamics of the system, which are described by the system model $f(t, \vx, \vu)$. Thus, the quality
of the controller heavily depends on the prediction accuracy of this model \cite{lucia2013}.
The system states however should remain in a set of constraints at all times whereas the inputs
are bounded, all summarized by the constraint functions $F_i$. In a physical sense, constraint violation can lead to 
instability and thus damage of the real system. Here is a vital example.
For a chemical reactor a \emph{reaction runaway}
must be avoided at all costs. This happens, if the exothermic heat release exceeds the maximum possible
cooling rate. Higher temperatures increase the reaction rate exponentially leading to a greater 
enthalpy release, which again causes a temperature rise. That is why, stability and guaranteed mathematical constraint satisfaction
are a great field of research in the discipline of optimal control \cite{}.


\section{Multi-Stage Model Predictive Control}

Coming from this general perspective on robust optimization and basic MPC, \emph{robust MPC} specializes
\emph{robustness} to a dynamic closed-loop control environment. 

\begin{definition} Continuous-time formulation of an MPC task under uncertainty.
    \label{theory:def: continuous mpc}
    \[
        \begin{gathered}
            \underset{\vu}{\min} \; \underset{\vw}{\max} \quad  J(t_0, t_\text{end}, \vx, \vu, \vw) \\
            \begin{aligned}
                \text{s.t.} \quad &\dot{\vx} = f(t, \vx, \vu, \vw) \\
                & F_i(t, \vx, \vu, \vw) \leq 0 \quad \forall t \in [t_0, t_\text{end}], \; \forall \vw \in \mathbb{W} \\
                & \vx_0 = \vx_\text{init}
            \end{aligned}
        \end{gathered}
    \]
\end{definition}

The only difference to the basic MPC formulation (\ref{theory:def: general mpc}) is that the model, the objective 
and the constraints are functions of the uncertainty, denoted with $\vw$.
This MPC formulation (def.~\ref{theory:def: continuous mpc}) is now infinite in time and in the uncertain parameters.
To make computation feasible,
the problem can be discretized in the time dimension eiher using single shooting, multiple shooting or orthogonal collocation
on finite elements \cite{biegler2010}. One possibility to roughly discretize the uncertain parameter set $\mathbb{W}$ is
done by \emph{multi-stage-MPC} \cite{lucia2015}. 
\newline
\newline

It generates a scenario tree (fig.~\ref{}) for all 
possible combinations of disturbances. If the uncertainty is discrete, this formulation is exact.
In most cases however, when the uncertainties lie in a continuous space, the scenario tree
is an extremely rough approximation. With a non-linear system, the scenarios are not monotonous regarding the
disturbance scenarios. This means, even if the scenario branches cover the extreme values of the disturbances
themselves, they can leave extreme scenarios caused by intermediate parameter combinations. Robust 
constraint satisfaction is not guaranteed and must be checked via a reachability analysis \cite{lucia2015}.
Another drawback of multi-stage MPC is, that it includes redundant scenarios not causing extreme cases due to 
the non-monotonicity. These scenarios still are part of the optimization leading to unnecessary complexity.
On top of this, the number of optimization variables growths exponentially with the \emph{robust horizon} $N_\text{r}$,
the number of parameters and their respective values (eq.~\ref{theory:eq: number scenarios}) \cite{empty000}. 

\begin{equation}
    \label{theory:eq: number scenarios}
   n_\text{s} = \bigg ( \prod^{n_\text{w}}_i \nu_i \bigg ) ^{N_{\text{r}}}
\end{equation}

Here, $n_\text{w}$ denotes the number of different disturbances and $\nu_i$ their respective discrete values. $n_\text{s}$
describes the number of scenarios in the scenario tree (fig.~\ref{}).



\section{Conservativeness and Performance}




\section{Neural Networks}
 
\emph{Feed forward neural networks} (NNs), sometimes referred to as \emph{multi-layer-perceptrons},
provide a stunning capability of approximating any multivariate function from any finite dimensional
space to another. According to the \emph{Universal approximation theorem}, any finite dimensional function can be reproduced if the number of hidden
layers and respective neurons is high enough, up to an arbitrarily small error \cite{hornik1990}. Thus, NNs play a major role in
recent developments in machine learning. The breakthrough of large language models such as ChatGPT heavily rely on neural
networks \cite{vaswani2023}. NNs are not only succesfully applied in natural language processing but in the chemical process industry.
Applications range from thermodynamic property prediction over unit operation design to corporate level decision making \cite{mcbride2019}.
Their ability to approximate any non-linear function while beeing differentiable, makes NNs especcialy suitable for dynamic optimization
such as MPC. In contrast to detailed first principle models, NNs have the advantage of making a prediction in a fraction of the inference time
of the first principle model. As MPC operates in real time parallel to the system to be controlled, the computation duration is a key measure
for a succesfull MPC implementation. That is why, surrogate models in form of NNs are a crucial tool for dynamic optimization and heavily investigated
in current research \cite{salzmann2024l4casadi}. May a NN be given as an alternating series of function evaluations (def.~\ref{theory:def: neural network}).

\begin{definition} Mathematical formulation of a neural network as a nested function evaluation.
    \label{theory:def: neural network}
    \[
    \begin{aligned}
        \mathcal{NN}(\bm{\xi, \bm{\theta}}) &= g_{l+1} \circ h_{l+1} \circ ... \circ g_1 \circ h_1(\bm{\xi})
        \\
        h(\bm{a}_{l-1}) &= \bm{\theta}_{Wl}\bm{a}_{l-1} + \bm{\theta}_{bl}
        \\
        \bm{a}_{l} &= g_l(h(\bm{a}_{l-1}))
    \end{aligned}
    \]
\end{definition}

In fact, a NN is a series of function combinations denoted by the $\circ$-symbol alternating between a linear function $h(\bm{a})$
and a \emph{non-linear} activation function $g(h(\va))$. The linear transformation matrices $\bm{\theta}_{Wl}$ and offsets $\bm{\theta}_{bl}$,
are called weights and biases respectively and are the trainable parameters of a NN. The activation functions $g$ are important
to introduce non-linearity into every linear projection. Otherwise, the complete function combination can be written as one 
singular linear transformation and the universal approximations capability is lost. Given a set of training data,
the optimal parameters for $\bm{\theta}$ are found by solving an optimization task that involves the minimzation of a prediction loss function (eq.~\ref{eq: loss function}).

\begin{equation}
    \label{eq: loss function}
    \begin{aligned}
        \mathbb{X}_\text{train} &\coloneq \{ \bm{\xi}^{(0)}, \hdots, \bm{\xi}^{(m)} \}
        \\
        \mathbb{Y}_\text{train} &\coloneq \{ \bm{y}^{(0)}, \hdots, \bm{y}^{(m)} \} 
        \\
        \mathcal{L}(\bm{\theta}, \mathbb{X}_\text{train}, \mathbb{Y}_\text{train}) &=
        \frac{1}{m} \sum_{i=0}^{m-1} \Big| \Big| \bm{y}^{(i)} - \bm{\tilde{y}}(\bm{\theta}, \bm{\xi}^{(i)}) \Big | \Big|_2^2
    \end{aligned}
\end{equation}

In \emph{supervised learning}, the training data is separated in \emph{features} $\mathbb{X}_\text{train}$, the inputs of the NN,
and \emph{labels} $\mathbb{Y}_\text{train}$, the outputs of the NN. During training, the parameters $\bm{\theta}$ are determined such that
the NN approximates the data in best possible manner.
In this example the loss function $\mathcal{L}$ portrays the mean squarred error (MSE), that is used in most regression tasks. 
The difference between the model prediction, denoted as $\bm{\tilde{y}}(\bm{\xi}^{(i)})$, and the true data point $\bm{y}^{(i)}$ is penalized quadratically.

\subsection{Quantile Regression}
By utilizig a loss function in form of the MSE, a model is trained to achieve regression predictions in form of the quarred mean.
To tweak the prediction characteristics of a NN for the task at hand, the type of training loss function is decisive.
In a probabilistic environment, the prediction of confidence intervalls in the form of quantiles may be of mayor interest.
To learn the quantiles of a distribution of data points, the \emph{quantile loss} or \emph{pinball loss} can be used (eq.~\ref{theory:eq: quantile loss}).

\begin{equation}
    \label{theory:eq: quantile loss}
    \mathcal{L}_\tau = 
    \begin{cases}
        \tau \cdot (y - \tilde{y}(\bm{\theta})), & y \geq \tilde{y}(\bm{\theta}) \\
        (1 - \tau) \cdot (\tilde{y}(\bm{\theta}) - y), &y < \tilde{y}(\bm{\theta})
    \end{cases}
\end{equation}

The hyperparameter $\tau$ denotes the type and therefore position of the quantile. A small value for $\tau$, e.g.
$0.05$ or $0.1$, strongly penalizes data points that are below the threshold $y < \tilde{y}(\bm{\theta})$ via the $(1-\tau)$-term
to keep the quantile small. A high value for $\tau$, e.g. $0.95$, focuses on the penalization of data points above
the threshold $y \geq \tilde{y}(\bm{\theta})$ to cover a greater amount of points.


\section{NARX Models}

Because of their universal approximation capability and their differentiability, NNs are well suited
as \emph{surrogate models} in MPC algorithms to approximate the system behavior represented by the function $f$ (def.~\ref{theory:def: general mpc}).
This can significantly reduce the solution time in contrast to discretized systems of ordinary differential equations (ODEs) \cite{}.
\begin{definition} Time-discrete form of a NN to predict the future state, based on past information contained in the vector $\bm{\xi}_k$.
    \label{theory:def: NARX Model}
    \[
    \begin{aligned}
        \vx_{k+1} &= f(\vx_k) \\
        \bm{\tilde{x}}_{k+1} &= \mathcal{NN}(\bm{\xi}_k)
    \end{aligned}
    \]
\end{definition}



