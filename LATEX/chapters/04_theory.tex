\chapter{Model Predictive Control under Uncertainty}


Before diving into the application of a special model predictive (MPC) algorithm. Uncertain
optimization is treated more generally. From economics, over self-driving cars to chemical plants,
optimization problems are formulated in terms of uncertain variables in countless variations.
These uncertainties arise in three different types – fluctuating disturbances such as wind or weather conditions,
parametric offsets such as wrongly determined friction coefficients or model mismatches like unseen side reactions.
As the straight maximization or minimzation of an objective often puts a problem to its boundaries,
unforeseen disturbances may lead to \emph{constraint violation} or even \emph{instability} \cite{houska2011}.
Therefore, \emph{robust} optimization formulations are needed, that are proof against mentioned challenges as
well as retain the \emph{performance} with respect to the objective function.
\newline

\section{Robust Optimization}

Considering the following optimization task (\ref{theroy:def: semi-infinite program}) under an uncertainty vector $\vw$.
\begin{definition} Semi-infinite program with the uncertainty $\vw$.
    \label{theroy:def: semi-infinite program}
    \[
    \begin{gathered}
        \underset{\vu}{\min} \; \underset{\vw}{\max} \quad F_0(\vu, \vw) \\
        \text{s.t.} \quad F_i(\vu, \vw) \leq 0 \quad \forall \vw \in \mathbb{W}
    \end{gathered}
    \]
\end{definition}

Here, the optimization of the objective is formulated in a bilevel form. The scalar objective function $F_0$ is to 
be minimized w.r.t the decision variables $\vu$ for the worst case scenario w.r.t the disturbance
vector $\vw$. It can be interpreted as a player-vs-player setup \cite{houska2011}. One player (the optimizer)
seeks a minimum solution for the input $\vu$, while the opposing player (nature) chooses a maximum solution for the
disturbance variables $\vw$. The inequality constraints $F_i$ in addition, should be satisfied for all possible disturbances, that
lie in a defined set $\mathbb{W}$. If this set is not finite, the number of possible 
constraints to be satisifed is infintely big. Thus, this formulation is named as \emph{semi-infinite-program} \cite{stein2003}.
To reduce the number of constraints, the worst-case estimate can be applied here (\ref{theroy:def: bi-level program}). 

\begin{definition} Bi-level formulation of the semi-infinite program
    \label{theroy:def: bi-level program}
    \[
    \begin{gathered}
        \underset{\vu}{\min} \; \underset{\vw}{\max} \quad F_0(\vu, \vw) \\
        \text{s.t.} \quad \varphi_i(\vu) \coloneq \underset{\vw}{\max} \quad F_i(\vu, \vw) \leq 0
        \quad \vw \in \mathbb{W}
    \end{gathered}
    \]
\end{definition}

Now, the inequality constraints are also formulated in a bi-level manner. The maximum of $F_i$ is 
constant in $\vw$ and only a function of the decision variables $\vu$. The function $\varphi_i$
exactly describes the wost-case for $F_i$ parametrized by $\vu$. In fact, if certain assumptions hold
regarding the uncertainty set $\mathbb{W}$ and the concavity of the maximization,
$\varphi_i$ can be exactly computed \cite{diehl2025}. This would simplify the problem dramatically.
In many real cases however, an analytical solution for $\varphi_i$ is impossible to compute.
But with the universal approximation capability of neural networks (NNs) in mind \cite{hornik1990}, 
a data-driven approach is promising, that delivers an approximated function $\tilde{\varphi}_i(\vu)$.


\section{Model Predictive Control}

Model predictive control (MPC) is a type of optimization, in which an optimal control problem is solved
in a dynamic manner. The main goal of this control strategy is to find the best inputs to a system that
maximize or minimize a defined objective. In a chemical process, for instance, the energy consumption
for heating should be minimized. It should be minimized in such a way, that other variables remain 
within their bounds. For example, the product specification and, most importantly, the safety of the process
needs to be ensured under all circumstances. In contrast to the \emph{optimal design} of a process, that 
is done during the engineering phase, an MPC controller operates \emph{online}. It acts on the process
while it is running. Thus, it is coined \emph{dynamic optimization}. To do so, the optimization
problem needs to solved recursively at given time instants.
Chemical plants usually have
large \emph{time constants}, meaning, they react slowly to their inputs.
Other physical systems with low time constants, like quadrocopters behave much faster. So, the
computation time of an MPC algorithm is to be kept low \cite{salzmann2023} to be able to react to the system.
To predict the future behavior of the system, MPC requires a system model.
\newline
\newline
In total, an MPC controller is based on three pillars – an objective function, a set of constraints and, crucially,
the model of the system.

\begin{definition} Continuous-time formulation of an MPC problem.
    \label{theory:def: general mpc}
    \[
        \begin{gathered}
            \underset{\vu}{\min} \quad  J(t_0, t_\text{end}, \vx, \vu) \\
            \begin{aligned}
                \text{s.t.} \quad &\dot{\vx} = f(t, \vx, \vu) \\
                & F_i(t, \vx, \vu) \leq 0 \quad \forall t \in [t_0, t_\text{end}] \\
                & \vx_0 = \vx_\text{init}
            \end{aligned}
        \end{gathered}
    \]
\end{definition}

Optimal system inputs $\vu$ are to be determined, that minimize the objective function $J$. This objective
varies with the inputs and the states of the system $\vx$. It is to be minimized over a time horizon
from $t_0$ to $t_\text{end}$, called the \emph{prediction horizon}. The states of the system evolve along
the dynamics of the system, which are described by the system model $f(t, \vx, \vu)$. Thus, the quality
of the controller heavily depends on the prediction accuracy of this model \cite{lucia2013}.
The system states however should remain in a set of constraints at all times whereas the inputs
are bounded, all summarized by the constraint  functions $F_i$. In a physical sense, constraint violation can lead to 
instability and thus damage of the real system. Here is a vital example.
For a chemical reactor a \emph{reaction runaway}
must be avoided at all costs. This happens, if the exothermic heat release exceeds the maximum possible
cooling rate. Higher temperatures increase the reaction rate exponentially leading to a greater 
enthalpy release, which again causes a self-accelerating temperature rise \cite{guttel2021}. 
That is why, stability and guaranteed mathematical constraint satisfaction
are a great field of research in the discipline of optimal control.


\section{Multi-Stage Model Predictive Control}

Coming from this general perspective on robust optimization and basic MPC, \emph{robust MPC} specializes
\emph{robustness} to a dynamic closed-loop control environment. 

\begin{definition} Continuous-time formulation of an MPC task under uncertainty.
    \label{theory:def: continuous mpc}
    \[
        \begin{gathered}
            \underset{\vu}{\min} \; \underset{\vw}{\max} \quad  J(t_0, t_\text{end}, \vx, \vu, \vw) \\
            \begin{aligned}
                \text{s.t.} \quad &\dot{\vx} = f(t, \vx, \vu, \vw) \\
                & F_i(t, \vx, \vu, \vw) \leq 0 \quad \forall t \in [t_0, t_\text{end}], \; \forall \vw \in \mathbb{W} \\
                & \vx_0 = \vx_\text{init}
            \end{aligned}
        \end{gathered}
    \]
\end{definition}

The only difference to the basic MPC formulation (\ref{theory:def: general mpc}) is that the model, the objective 
and the constraints are functions of the uncertainty, denoted with $\vw$.
This MPC formulation (def.~\ref{theory:def: continuous mpc}) is now infinite in time and in the uncertain parameters.
To make computation feasible,
the problem can be discretized in the time dimension eiher using single shooting, multiple shooting or orthogonal collocation
on finite elements \cite{biegler2010}. One possibility to roughly discretize the uncertain parameter set $\mathbb{W}$ is
done by \emph{multi-stage-MPC} \cite{lucia2015}. 
\newline
\newline

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{Figures/Theory/multi-stage-mpc.pdf}
    \caption[Multi-stage MPC scenario tree]
    {Multi-stage MPC discretizies the uncertainty set via user-defined possible cases $w_k^{(\cdot)}$.
    All possible case combinations are branched through the robust horizon $N_\text{r}$. To limit the 
    complexity, the uncertainties are considered constant thereafter for the rest of the prediction
    horizon $N_\text{p}.$
    }
    \label{theory:fig: multi-stage-mpc}
\end{figure}

It generates a scenario tree (fig.~\ref{theory:fig: multi-stage-mpc}) for all 
possible combinations of disturbances. In the algorithm design phase, scenario cases $w_k^{(\cdot)}$ must be
defined a priori for each independent disturbance that may act on the system. As different disturbances occur
in different combinations, each possible scenario is represented by a branch. At every time step in the \emph{robust horizon},
branches for every possible uncertainty scenario are generated. This procedure is coined \emph{scenario generation}.
In practice, the worst-cases for the disturbances are assumed and sometimes an additional expected or nominal value. Mostly, this yields good results
with respect to controller performance and robustness \cite{lucia2013}.
\newline
\newline
If the uncertainty is discrete and all cases are considered, the scenario tree formulation (fig.~\ref{theory:fig: multi-stage-mpc}) is exact.
In most applications however, when the uncertainties lie in a continuous space, the scenario tree
is an extremely rough approximation. With a non-linear system, the scenarios are not monotonous regarding the
disturbances. This means, even if the scenario branches cover the extreme values of the disturbances
themselves, they can leave extreme scenarios caused by intermediate parameter combinations. Robust 
constraint satisfaction is not guaranteed and must be checked via a reachability analysis \cite{lucia2015}.
This can be an obstacle in safety-relevant applications.
Another drawback of multi-stage MPC is, that it includes redundant scenarios not causing extreme cases due to 
the non-monotonicity. These scenarios still are part of the optimization leading to unnecessary complexity.
On top of this, the number of optimization variables growths exponentially with the \emph{robust horizon} $N_\text{r}$,
the number of parameters and their respective values (eq.~\ref{theory:eq: number scenarios}) \cite{empty000}. 

\begin{equation}
    \label{theory:eq: number scenarios}
   n_\text{s} = \bigg ( \prod^{n_\text{w}}_i \nu_i \bigg ) ^{N_{\text{r}}}
\end{equation}

Here, $n_\text{w}$ denotes the number of different disturbances and $\nu_i$ their respective discrete values. $n_\text{s}$
describes the number of scenarios in the scenario tree (fig.~\ref{theory:fig: multi-stage-mpc}).
High order uncertainties lead to optimization problems that are simply impossible to solve in real time.



\section{Neural Networks}
 
\emph{Feed forward neural networks} (NNs), sometimes referred to as \emph{multi-layer-perceptrons} (MLPs),
provide a stunning capability of approximating any multivariate function from any finite dimensional
space to another. According to the \emph{Universal Approximation Theorem}, any finite dimensional function can be reproduced if the number of hidden
layers and respective neurons is high enough, up to an arbitrarily small error \cite{hornik1990}. Thus, NNs play a major role in
recent developments in machine learning. The breakthrough of large language models such as ChatGPT heavily rely on neural
networks \cite{vaswani2023}. NNs are not only succesfully applied in natural language processing but in the chemical process industry.
Applications range from thermodynamic property prediction over unit operation design to corporate level decision making \cite{mcbride2019}.
Their ability to approximate any non-linear function while beeing differentiable, makes NNs especcialy suitable for dynamic optimization
such as MPC. In contrast to detailed first principle models, NNs have the advantage of making a prediction in a fraction of the inference time
of the first principle model. As MPC operates in real time parallel to the system to be controlled, the computation duration is a key measure
for a succesfull MPC implementation. That is why, surrogate models in form of NNs are a crucial tool for dynamic optimization and heavily investigated
in current research \cite{salzmann2024l4casadi}. May a NN be given as an alternating series of function evaluations (def.~\ref{theory:def: neural network}).

\begin{definition} Mathematical formulation of a neural network as a nested function evaluation.
    \label{theory:def: neural network}
    \[
    \begin{aligned}
        \mathcal{NN}(\bm{\xi, \bm{\theta}}) &= g_{l+1} \circ h_{l+1} \circ ... \circ g_1 \circ h_1(\bm{\xi})
        \\
        h(\bm{a}_{l-1}) &= \bm{\theta}_{Wl}\bm{a}_{l-1} + \bm{\theta}_{bl}
        \\
        \bm{a}_{l} &= g_l(h(\bm{a}_{l-1}))
    \end{aligned}
    \]
\end{definition}

In fact, a NN is a series of function combinations denoted by the $\circ$-symbol alternating between a linear function $h(\bm{a})$
and a \emph{non-linear} activation function $g(h(\va))$. The linear transformation matrices $\bm{\theta}_{Wl}$ and offsets $\bm{\theta}_{bl}$,
are called weights and biases respectively and are the trainable parameters of a NN. The activation functions $g$ are important
to introduce non-linearity into every linear projection. Otherwise, the complete function combination can be written as one 
singular linear transformation and the universal approximation capability is lost. Given a set of training data of $m$ data points,
the optimal parameters for $\bm{\theta}$ are found by solving an optimization task that involves the minimzation of a prediction loss function (eq.~\ref{eq: loss function}).

\begin{equation}
    \label{eq: loss function}
    \begin{aligned}
        \mathbb{X}_\text{train} &\coloneq \{ \bm{\xi}^{(0)}, \hdots, \bm{\xi}^{(m-1)} \}
        \\
        \mathbb{Y}_\text{train} &\coloneq \{ \bm{y}^{(0)}, \hdots, \bm{y}^{(m-1)} \} 
        \\
        \mathcal{L}(\bm{\theta}, \mathbb{X}_\text{train}, \mathbb{Y}_\text{train}) &=
        \frac{1}{m} \sum_{i=0}^{m-1} \Big| \Big| \bm{y}^{(i)} - \bm{\tilde{y}}(\bm{\theta}, \bm{\xi}^{(i)}) \Big | \Big|_2^2
    \end{aligned}
\end{equation}

In \emph{supervised learning}, the training data is separated in \emph{features} $\mathbb{X}_\text{train}$, the inputs of the NN,
and \emph{labels} $\mathbb{Y}_\text{train}$, the outputs of the NN. During training, the parameters $\bm{\theta}$ are determined such that
the NN approximates the data in best possible manner.
In this example the loss function $\mathcal{L}$ portrays the mean squarred error (MSE), that is used in most regression tasks. 
The difference between the model prediction, denoted as $\bm{\tilde{y}}(\bm{\xi}^{(i)})$, and the true data point $\bm{y}^{(i)}$ is penalized quadratically.

\subsection{Quantile Regression}
By utilizig a loss function in form of the MSE, a model is trained to achieve regression predictions in form of the quarred mean.
To tweak the prediction characteristics of a NN for the task at hand, the type of training loss function is decisive.
In a probabilistic environment, the prediction of confidence intervalls in the form of quantiles may be of mayor interest.
To learn the quantiles of a distribution of data points, the \emph{quantile loss} or \emph{pinball loss} can be used (eq.~\ref{theory:eq: quantile loss}).

\begin{equation}
    \label{theory:eq: quantile loss}
    \mathcal{L}_\tau = 
    \begin{cases}
        \tau \cdot (y - \tilde{y}(\bm{\theta})), & y \geq \tilde{y}(\bm{\theta}) \\
        (1 - \tau) \cdot (\tilde{y}(\bm{\theta}) - y), &y < \tilde{y}(\bm{\theta})
    \end{cases}
\end{equation}

The hyperparameter $\tau$ denotes the type and therefore position of the quantile. A small value for $\tau$, e.g.
$0.05$ or $0.1$, strongly penalizes data points that are below the threshold $y < \tilde{y}(\bm{\theta})$ via the $(1-\tau)$-term
to keep the quantile small. A high value for $\tau$, e.g. $0.95$, focuses on the penalization of data points above
the threshold $y \geq \tilde{y}(\bm{\theta})$ to cover a greater amount of points.
For robust optimization, \emph{quantile regression} can be applied to learn upper or lower bounds for the uncertainty-dependent
constraints $F_i(\vu, \vw)$ (eq.~\ref{theroy:def: bi-level program}) in terms of confidence quantiles. This would reduce
the semi-infinite program to an uncertainty-finite-program.


\section{NARX Models}

Because of their universal approximation capability \cite{hornik1990} and their differentiability, NNs are well suited
as \emph{surrogate models} in MPC algorithms to approximate the system behavior represented by the function $f$ (def.~\ref{theory:def: general mpc}).
This can significantly reduce the solution time in contrast to discretized systems of ordinary differential equations (ODEs) \cite{salzmann2023}.

\begin{definition} Time-discrete form of a NN to predict the future state, based on past information.
    \label{theory:def: NARX Model}
    \[
    \begin{aligned}
        \vx_{k+1} &= f(\bm{x}_k, \vu_k) \\
        \bm{\tilde{x}}_{k+1} &= \mathcal{NN}(\vx_k, \vx_{k-1}, ..., \vx_{k-h}, \vu_k, \vu_{k-1}, ..., \vu_{k-h})
    \end{aligned}
    \]
\end{definition}

In the discrete-time notation, the state at the next time instant $\vx_{k+1}$ is calculated by propagating the current
state $\vx_k$ trough a function $f(\vx_k, \vu_k)$ that discribes the system dynamic (def.~\ref{theory:def: NARX Model}).
In the original MPC formulation (def.~\ref{theory:def: general mpc}) this is some numerical integration scheme for a system of differential equations \cite{biegler2010}
with the abbreviation \emph{right-hand-side} (rhs).
The state propagation by the function $f$ is a classical example of a time series extrapolation. NNs have shown significant
capabilities in time series forecasting. Deep learning based architectures exist for time series forecasting
in various complexities. Convolutional NNs (CNNs), Long-Short-Term-Memory (LSTMs) cells and transformers are the most promiment for
time series prediction with varying complexity \cite{benidis2022,vaswani2023}.
These provide a great possibility to apply deep learning based forecasting within optimal control algorithms, where the next
state $\bm{\tilde{x}}_{k+1}$ is the output of a neural network $\mathcal{NN}$.
In context of this work, shallow feedforward NNs or \emph{single-layer perceptrons} (SLPs) are used for this purpose which
technically do belong to \emph{shallow learning} architectures.
\newline
\newline
In the realm of control, the non-linear system indendification is summarized with the term
\emph{Non-linear AutoRegressive with eXogenous inputs} (NARX). Here, the term \emph{autoregressive} means, that
the future prediction is a function of a series of past system states \{$\vx_k, \vx_{k-1}, ..., \vx_{k-h}$\}, while \emph{exogenous}
implies, that a series of outside inputs \{$\vu_k, \vu_{k-1}, ..., \vu_{k-h}$\} is passed to the function as well (def.~\ref{theory:def: NARX Model}).
The hyperparameter $h$ denotes the length of the time horizon of the past series. A high value for this parameter includes more
past information, which possibly allows better predictions while increasing the network input.
Between states and inputs, $h$ can be choosen differently. In this work, $h$ is set equal for states and inputs. 

\subsection{Input Compression}
\begin{equation}
    \label{theory:eq: input feature vector}
    \bm{\phi}_k \coloneq (\vx_k, \vx_{k-1}, ..., \vx_{k-h}, \vu_k, \vu_{k-1}, ..., \vu_{k-h})
\end{equation}

The concatenated input feature vector $\bm{\phi}_k$ contains all past states and inputs within the time horizon $h$, leading
to a vector length of $(n_x + n_u) \cdot (h+1)$, where $n_x$ and $n_u$ represent the number of states and inputs respectively. 
For large systems, the input length increases the network size unaccetably quick, while the input time serieses often exhibit
strong redundacies. To reduce the input size, the feature vector $\bm{\phi}_k$ is passed to an \emph{encoder} \cite{kemmerling2025}.
The encoder is based on feature compression via \emph{principal component analysis} (PCA), that maps the feature space to 
a lower dimensional subspace. This subspace represents the space of the input data with the most information.

\subsection{Singular Value Decomposition}
\emph{Singular value decomposition} (SVD) is a fundamental building block in linear algebra. Its applications 
are range from image compression, over exploratory data analysis to model reduction in fluid mechanics.
The power of SVD is, that it maps a high dimensional space into a low dimensional subspace. The mapping is achieved by a simplistic matrix
transformation. It not only identifies a subspace, it also identifies underlying patterns in the data encoded
via the respective \emph{singular vectors}. The compression and component indendification is guaranteed
to exist for every matrix $\bm{X} \in \mathbb{C}^{n \times m}$ in contrast to an eigendecomposition \cite{Brunton2019}.
\newline
\newline
The SVD is extremely useful, when high dimensional data is presented, that embeds some kind of pattern. For example,
simulations of partial differential equations (PDEs), images as collections of pixels and time serieses exhibit strong patterns within the data.
This is especcialy true for data points in close temporal or spatial distance. Because the SVD is the basis for
PCA to compress the feature vector $\bm{\phi}_k$ and \emph{model order reduction} (MOR) of ODEs
or PDEs it is explained in further detail. According to the SVD, any matrix $\bm{X} \in \mathbb{C}^{n \times m}$ can be decomposed
into three submatrices (th.~\ref{theory:th: singular value decomposition}).
\begin{theorem} Singular value decomposition of a data matrix $\bm{X}$.
    \label{theory:th: singular value decomposition}
    \[
    \bm{X} = \bm{U} \bm{\Sigma} \bm{V}^*
    \] 
\end{theorem}

Here, the matrices $\bm{U} \in \mathbb{C}^{n \times n}$ and $\bm{V}^* \in \mathbb{C}^{m \times m}$ are quadratic, orthonormal matrices.
The real, non-negative, diagonal matrix $\bm{\Sigma} \in \mathbb{R}^{n \times m}$ contains the so called \emph{singular values} of the matrix
with the biggest as the top-left entry. The notation $\bm{V}^*$ stands for the transposed complex-conjugate of $\bm{V}$.
The computation in \texttt{python} can be done in \texttt{numpy} via the method \texttt{linalg.svd()} or
directly in a training environment with \texttt{torch.linalg.svd()}. For large data matrices $\bm{X}$, that do not
fit into memory, a batched or incremental SVD must be considered. A current \texttt{torch} implementation with GPU support
exists under \texttt{torch\_incremental\_pca} \cite{incrementalpca2025}.
The compression itself is done by truncating the yielded matrices by taking the first $r$ columns of $\bm{U}$, the
first $r$ rows of $\bm{V}^*$ and $r$ diagonal elements of $\bm{\Sigma}$ (eq.~\ref{theory:eq: truncated svd}).
\begin{equation}
    \label{theory:eq: truncated svd}
    \begin{gathered}
        \bm{X} \approx \bm{\Psi} \bm{\tilde{\Sigma}} \bm{\tilde{V}}^* \\
        \text{with} \quad \bm{\Psi} \coloneq \bm{U}_{:,:r} \quad \bm{\tilde{\Sigma}} = \bm{\Sigma}_{:r,:r} \quad \bm{\tilde{V}}^* = \bm{V}^*_{:r,:}
    \end{gathered}
\end{equation}
This has the advantage, that an approximation of the data $\bm{X}$ can be stored via the truncated matrices $\bm{\Psi}, \bm{\tilde{\Sigma}}, \bm{\tilde{V}}^*$
which take up less memory than the original matrix. It also allows to project the original matrix into a subspace
of dimension $r$ (eq.~\ref{theory:eq: svd projection}).
\begin{equation}
    \label{theory:eq: svd projection}
        \bm{\tilde{X}} = \bm{\Psi}^\top \bm{X} \Longleftrightarrow \bm{X} \approx \bm{\Psi} \bm{\tilde{X}}
\end{equation}
The truncated matrix $\bm{\Psi}$ acts as the transformation matrix between the low dimensional space and the original space (eq.~\ref{theory:eq: svd projection}).
It is important to note, that the back transformation into the original space only yields an approximation of the original data.
It can be prooven, that this approximation is the best existing one with respect to the Frobenius Norm as it solves following minimization \cite{Brunton2019} (eq.~\ref{theory:eq: svd minimization}).
\begin{equation}
    \label{theory:eq: svd minimization}
    \begin{gathered}
        \bm{\Psi} = \underset{\bm{\Psi}}{\text{argmin}} \quad ||\bm{X} - \bm{\Psi} \bm{\Psi}^\top \bm{X}||_F \\
        \text{s.t.} \quad \text{rank}(\bm{\Psi}) = r
    \end{gathered}
\end{equation}
Reentering the concept of neural network NARX models, the large vector of input features $\bm{\phi}_k$ (eq.~\ref{theory:eq: input feature vector})
can be compressed via SVD into a low dimensional feature vector $\bm{\xi}_k$ that retains the most 
dominant basis vectors of the original input (fig.~\ref{theory:fig: narx-pca-encoder}). This is done 
by an \emph{PCA encoder} placed before the first layer of a NARX model to reduce the network size. 

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{Figures/Theory/narx-pca-encoder.pdf}
    \caption[PCA encoder for feature reduction]
    {The PCA encoder uses SVD to derive a projection matrix $\bm{\Psi}$ that maps the high dimensional feature vector
    $\bm{\phi}_k$ to a lower dimensional feature vector $\bm{\xi}_k$ which is fed into a NARX model. This reduces the network size significantly,
    while retaining the most important information about the input.
    }
    \label{theory:fig: narx-pca-encoder}
\end{figure}

\section{Conservation of Atoms}

The derived methology will later be applied to control a \emph{packed-bed tubular reactor} under parametric uncertainty which is
a common realization for many chemical processes. For this sake, the \emph{conservation of atoms} during chemical reactions
is introduced. Similar to the conservation of mass or energy, the conservation of atoms demands, that all atoms that enter
a chemical reaction must also leave this reaction. While they change the species they form, the atoms themselves are conserved.
This can be expressed as follows.
\begin{theorem} Conservation of atoms during a chemical reaction.
    \label{theory:th: conservation of atoms}
    \[
    \bm{B} \bm{\Delta n} = \bm{0}
    \]
\end{theorem}

Here, $\bm{B}$ denotes the \emph{element-species-matrix} and $\bm{\Delta n}$ the changes in the molar amounts of all participating
specieses in a reaction network. The sum of all atoms in all species must be equal at all times regardless of how the underlying reactions take place.
This builds the foundation of reaction network analysis and the determination of key and non-key-components \cite{guttel2021}.
\begin{equation}
    \label{theory:eq: element-species-matrix}
    \bm{B} = \begin{pmatrix}
        \beta_{1, 1} & \hdots & \beta_{1, \alpha} \\
        \vdots       & \ddots & \vdots \\ 
        \beta_{h, 1} & \hdots & \beta_{h, \alpha}
    \end{pmatrix}   
\end{equation}
The element-species-matrix contains the elements $h$ as rows and the species $\alpha$ as columns. $\beta_{h, \alpha}$ describes
the number of atoms of element $h$ in the species $\alpha$. This implies, that when the initial molar amounts of all species are known,
all possible combinations of molar amounts must lie in a pre-defined space.

